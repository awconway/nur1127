---
title: 'Week 4: Non-randomized studies of interventions'
description:
  'The essential elements, advantages and disadvantages of non-randomized
  studies of interventions will be discussed. We will cover the biases that
  threaten the validity of results from these types of studies. In particular,
  you will be introduced to the statistical approaches used to account for
  confounding, which is a major consideration in the risk of bias assessment for
  non-randomized studies of interventions. You will learn to understand and
  apply criteria for assessing the quality of evidence from non-randomized
  studies of interventions using the GRADE framework.'
prev: /07sr
next: /05mm
type: chapter
id: 6
---

<exercise id="1" title="Introduction">

This week we continue towards our goal of being able to make recommendations
about whether or not to use a particular intervention/strategy/program in
practice by following the GRADE approach. Not all clinical questions/scenarios
will have evidence from a randomized controlled trial (or a meta-analysis of
randomized controlled trial) to inform our decision-making for clinical
practice. In fact, I would hazard a guess that in _most_ situations there are
no RCTs available. That means it is really important for us to be able to use
evidence from non-randomized studies. The GRADE framework provides a way for us
to integrate results from non-randomized studies into practice recommendations.

Non-randomized studies of interventions test cause-and-effect relationships, but
the lack of randomization threatens the study's internal validity and weakens
claims about causal inference. Many different designs fall within the broad
category of 'non-randomized studies of interventions'. The common types will be
discussed below.

Before we move into the specifics about different types of designs, it may be
helpful to consider the following scenario to help you understand why a
researcher may choose to undertake a non-randomized study at all, considering
the obvious limitations. Suppose you were interested in the effects of a new
discharge education program that is being introduced into your institution. You
could randomly assign participants to receive either the new program or the
usual program but there may be a number of practical or administrative reasons
why this is impossible within the constraints of the resources that are
available. As an alternative, you may decide it is more feasible to compare the
outcomes of your units' patients (the intervention group) with those of a
similar unit at a different institution that has not implemented the program
(the control group). The table below lists some advantages and disadvantages of
non-randomized studies of interventions.

| Advantages                                                                                       | Disadvantages                                                                                                       |
| ------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------- |
| Practical, feasible and able to be generalised.                                                  | Need to rule out any plausible alternative explanations for the findings—control by design or statistical analysis. |
| May be the only design to evaluate some hypotheses, particularly in clinical settings.           | Unable to make clear cause-and-effect inferences.                                                                   |
| More easily adapted to the real-world practice setting than randomized controlled trial designs. |                                                                                                                     |




</exercise>

<exercise id="2" title="Non-equivalent control group designs">

As you can see in the diagram below, this design is similar to a standard
randomized controlled trial design, except participants were not randomised to
study groups. The choice to not use a randomized design may have been due to
practical or feasibility issues. Despite the lack of randomisation, this design
is commonly used in clinical research. Collection of pre-intervention data
allows for a comparison of the two groups **before** the intervention is
introduced enables the researcher to control for differences during data
analysis. This feature strengthens the influence of the intervention, and
minimises any effects of extraneous variables.

![](non-equivalent-control-group.png)

For example,
[Kozlowski et al (2018)](https://www-sciencedirect-com.myaccess.library.utoronto.ca/science/article/pii/S0897189717307656)
used a non-equivalent control group design to evaluate an Emotional Intelligence
training program for nurses. The Emotional Intelligence self-assessment tool was
administered to an intervention and a control group at two time points, which
were three months apart. The intervention group received an Emotional
Intelligence training program which consisted of a five-hour workshop, a
30-minute one-on-one feedback session with a trainer, and an individualised
follow-up reminder SMS. It was found that Emotional Intelligence scores
significantly increased over baseline levels for the intervention group while
scores for the control group remained unchanged. Take a look at the study
results and answer the question below.

**What is the most concerning issue about the comparisons reported for the
Kozlowski study?**

<choice>

<opt text="Confidence intervals were not reported">

You are right that reporting confidence intervals is important for us to be able
to interpret what the likely effect is in the population, but there is a more
fundamental concern here.

</opt>

<opt text="The difference in EI scores <em>between</em> groups was not reported" correct="true">

Knowing that EI scores increased in one group of participants increased over
time gives us no indication of the _comparative_ effects of this intervention
against _not_ receiving the intervention. It simply showed that in this
particular group of individuals that their EI increased on average from the
interval between the pre and post test. It is possible that factors _other_ than
the education intervention could have explained why their scores increased. The
simple fact that the EI scores in the control group didn't increase doesn't give
us an estimate about the _difference_ in EI scores we should expect to see if we
were to implement the intervention in the population.

</opt>

<opt text="The differences between the control and intervention groups were not controlled for in an adjusted analysis">

You are right that this is a major limitation. The researchers could easily have
included some variables in a model to control for the potentially confounding
influence of the nurses education, years of work experience, age etc. However,
there is a more important issue with the comparisons reported in this study.

</opt>

</choice>

### After-only non-equivalent control group studies

It is important to note that, in some circumstances, a researcher may be unable
to measure specific characteristics of the participants before the introduction
of a new intervention, but they still wish to collect data demonstrating the
efficacy of the program/intervention. This type of opportunistic study can also
be referred to as a ‘natural experience’ because the participants (or groups of
participants) are exposed to the intervention or control based on factors
outside the control of the researchers. This design has the benefit of
minimizing ‘testing effects’ (whereby completing a pre-test affects the
post-test scores) but it is a weaker design because it assumes that the two
groups are equivalent before the intervention is introduced, which is likely not
to be the case because of the lack of randomization.

To demonstrate this design in the health services context, an after-only
non-equivalent control group design was used to opportunistically evaluate the
impact of an initiative to add unregulated nursing support workers to wards in
acute care hospitals
[(Duffield et al 2018)](https://onlinelibrary-wiley-com.myaccess.library.utoronto.ca/doi/full/10.1111/jocn.14632).
Five wards where nursing support workers had been introduced were compared to a
group of five different wards without nursing support workers. A cross sectional
survey was administered to staff and patients in both groups. The patient survey
consisted of Patient Evaluation of Emotional Care during Hospitalisation (PEECH)
tool and the staff survey consisted of the Practice Environment Scale. The
results showed that adding nursing support workers to ward staffing did not lead
to improvements in patient care or practice environment. The researchers
identified limitations with the study design and suggested the inclusion of
baseline measurements and matching should be used in future research to allow
for more robust conclusions to be drawn about the effects of this staffing
strategy.

</exercise>

<exercise id="3" title="Before-after designs">

Many studies use some type of 'before-after' design to evaluate the effects of
an intervention. One approach is to define a given sample, measure the outcome,
implement the intervention, and then measure the outcome again with that same
group of participants. This is referred to as a 'one-group pre-test post-test
study'. The lack of randomisation and a control group means that potentially
important confounding variables are not accounted for which significantly limits
the internal validity of the studies. Statistical analyses (i.e. an 'adjusted'
analysis) cannot be used to 'account' for potential confounding because of the
lack of a control group. The design is most frequently used to study the
effectiveness of educational programs, or the restructuring of social groups and
organisations, or the implementation of behavioral interventions. In nursing
research this design is most frequently use to evaluate education interventions.

![](pre-post.png)

For example,
[Lamont et al (2018)](https://www-sciencedirect-com.myaccess.library.utoronto.ca/science/article/pii/S0260691718302004)
used a pretest-posttest design to assess whether a workplace violence training
program for nurses in acute hospital settings improved risk assessment and
management practices, de-escalation skills, breakaway techniques, and confidence
levels. The ‘Confidence in Coping with Patient Aggression Instrument’ was
administered to participants before and after nurses completed the training
program. It was found that participants experienced significant improvements in
confidence scores on 14 of the 15 domains of the instruments. The researchers
acknowledged that the generalizability of the findings was limited because it
was conducted at a single site, with a small sample, and no control group.
Please read the study and answer the following question.

**Apart from changing the design to a randomized study, what could the
researchers have done to make the effect estimate more valid?**

<choice>

<opt text="'Adjust' for the potential confounding influence of variables associated with the outcome being measured (eg. prior experience with workplace violence)" correct="true">

Due to the non-randomized design that was used, we could never 'rule-out' the
potential that some other factor could have partially or fully explained the
changes in nurses' confidence levels. However, adjusting for variables that
would theoretically have influenced this outcome would provide us with a better
indication of the effect that this intervention. For example, those who may have
experienced workplace violence before may have gotten more out of the training
due to an ability to place what they were learning into a real context and hence
increased their confidence. The opposite could also have been true.

</opt>

<opt text="Used logistic regression">

The outcome used in this study was continuous. Logistic regression is for a
binary outcome.

</opt>

</choice>

An alternative version of a 'before-after' design is one in which the
participants in the pre-test group are different to those included in the
'post-test' group. In other words, the outcome is measured before and after
implementation of an intervention, but participants are different individuals
before the intervention to those involved after the intervention.

To illustrate, consider this study. A before-after design was used to determine
the impact of an evidence-based intervention (a Thermal Care Bundle) on the
prevention, detection and treatment of perioperative inadvertent hypothermia (a
temperature <36°C before, during, or after surgery)
[(Duff et al 2017)](https://onlinelibrary-wiley-com.myaccess.library.utoronto.ca/doi/full/10.1111/jocn.14171).
The incidence of perioperative hypothermia and compliance with the
evidence-based care bundle were collected at two time point before and after the
implementation of the intervention. The study found that the bundle improved the
management of perioperative inadvertent hypothermia through increased risk
assessment, temperature recording and active warming but did not impact on the
incidence of perioperative inadvertent hypothermia.

</exercise>

<exercise id="4" title="Interrupted time-series designs">

Another approach when only one group is available is to study that group over a
longer period, using a time-series design. In this design, the variable of
interest is measured at multiple time points with the intervention introduced at
a predetermined time-point (not necessarily the mid-point). If the intervention
has a casual effect then the mean, or the slope of the trend, of the pre
intervention data should be different to that of the post intervention data.

This type of design is becoming popular in health services research. A team of
nurse researchers from the UK used an interrupted time series design to
evaluated whether the introduction of portable nursing stations reduced
inpatient falls
[(Ali et al 2018)](https://academic.oup.com/ageing/article/47/6/818/5054440).
Data on the rates of inpatient falls were collected monthly across 17 wards for
24 months prior and 12 months following the introduction of portable nursing
stations. The study found that portable nursing stations were associated with
lower monthly falls rates. The slope of the pre-intervention data trend was
0.119 (95% CI 0.045 to 0.194) compared to -0.222 (95% CI -0.350 to -0.093) for
the post-intervention data which equated to statistically significant change of
0.341 (95% CI 0.159 to 0.524; p=0.001). The plot of the trend in fall-rates
quite clearly demonstrates this effect.

![](time-series-plot.png)

Source: U M Ali, A Judge, C Foster, A Brooke, K James, T Marriott, S E
Lamb, Do portable nursing stations within bays of hospital wards reduce the rate
of inpatient falls? An interrupted time-series analysis, Age and Ageing, Volume
47, Issue 6, November 2018, Pages 818–824

**What other types of health service outcomes do you think would be suited to
being studies with the interrupted time-series design?**

<choice>

<opt text="Pressure injuries">

</opt>

<opt text="Hospital acquired infections">

</opt>

<opt text="Medication errors">

</opt>

<opt text="All of the above" correct="true">

All of these types of outcomes are quite well-suited to the interrupted
time-series design because these events are often routinely measured. This makes
the research a little bit easier to execute if reliable data for the
'pre-intervention' period are readily available.

</opt>

</choice>

</exercise>

<exercise id="5" title="Risk of bias assessment for non-randomized studies of interventions">

<qu>You will need to use the Risk Of Bias In Non-randomised Studies - of
Interventions (ROBINS-I) tool to evaluate risk of bias if you include
non-randomized studies for the final assessment of this course</qu>

There are three readings about risk of bias assessment for this week. The first
is an overview of the ROBINS-I tool, including how it can be used used to assess
risk of bias for non-randomized studies of interventions.


[📘 ROBINS-I: a tool for assessing risk of bias in non-randomised studies of interventions](https://www-bmj-com.myaccess.library.utoronto.ca/content/355/bmj.i4919)

The second is the guidance document for ROBINS-I. You don't necessarily have to
read it cover to cover, but it provides detailed information that you can refer
to when undertaking risk of bias assessments.

[📘 ROBINS-I guidance document](https://drive.google.com/file/d/0B7IQVI0kum0kenN0SGktSnNHTEE/preview)

Finally, the ROBINS-I assessment of risk of bias will be a vital component to
making your GRADE recommendation for practice. As such, another reading for this
week is the article below that provides detailed information about how ROBINS-I
can be incorporated into a GRADE assessment. The point I would like to emphasise
in this regard is that although the generic guidance from GRADE is that evidence
from observational studies (of which non-randomized studies are a type) start
their rating at 'low-quality', due to the comprehesiveness of the ROBINS-I tool,
it is appropriate to start a GRADE assessment as 'high quality' if the ROBINS-I
tool is used. That said, the GRADE authors note (in the paper below), that it
would be highly unlikely that there would not be the requirement to downgrade
quality of evidence, based on the issues that arise during the course of a risk
of bias assessment using ROBINS-I (the conclusion of the paper below sums this
point up nicely).

[📘 GRADE guidelines: 18. How ROBINS-I and other tools to assess risk of bias in
nonrandomized studies should be used to rate the certainty of a body of evidence](https://www-sciencedirect-com.myaccess.library.utoronto.ca/science/article/pii/S0895435617310314)

</exercise>

<exercise id="6" title="Webinar">


<!-- Access [Zoom](https://utoronto.zoom.us/j/84161933658
) for the webinar held on Feb 1 at 12md.  -->

<iframe src="https://player.vimeo.com/video/672474134?h=5a8be8e820&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" width="100%" height="480" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>


A link to the study we will assess is below:

[🔗 Interruptions During Senior Nurse Handover in the Intensive Care Unit : A Quality Improvement Study](https://oce-ovid-com.myaccess.library.utoronto.ca/article/00001786-201901000-00018/HTML)

[🔗 Blank ROBINS-I template to download](https://drive.google.com/file/d/0B7IQVI0kum0kWldlU1BzRGxnclE/view)



📁 A copy of the completed ROBINS-I assessment that I used in the
webinar can be downloaded
[here](attachments/ROBINS-I_completed.docx).


</exercise>

<exercise id="7" title="Readings">

## Required

**Please ensure you have completed the readings in ROBINS-I in the section above
(preferably prior to attending/viewing the webinar). These can be a bit dense so
it will take some time. However, all the required explanation of terms are
included in those documents and I will demonstrate an example of how to use the
tool in the webinar.**

Gray, J.R., Grove, S.K. & Sutherland, S. (2017). The Practice of Nursing
Research (8th ed.) Chapter 11: Quantitative Methodology: Interventional Designs
and Methods. (pp. 217-250).

Lancaster, G.A. & McCray, G. (2020). Using evidence from quantitative studies.
In J.V. Craig & D. Dowding (Eds.) Evidence-Based Practice in Nursing (4th ed.) (72-92).

</exercise>
