---
title: 'Week 12: Machine learning and prediction'
description:
  'You will be introduced to some of the common approaches used in healthcare research that applies machine learning techniques.'
prev: /11ethics
next: null
type: chapter
id: 12
---

<exercise id="1" title="Overview of prediction using 'machine learning'">

You will undoubtedly be hearing more and more about machine learning (ML) and artificial intelligence applications in healthcare. For this reason, I thought it would be good to introduce you to some of the common approaches currently being applied in healthcare research that draws on machine learning techniques.

Central machine learning applications in healthcare research is the concept of **prediction**. In very simple terms, prediction in machine learning means that we use what we can 'learn' from data drawn from one sample of the population, and then use that information to make a 'guess' about what will be observed in a *different sample from the same population*.

Simple 'statistical' models that we have already learned about can be used for just this purpose. You'll recall from NUR1027 when we covered [interpreting regression coefficients](https://nur1027.netlify.com/08stats), that we can quantify the relationship between an *outcome* and *predictor* variables using linear regression by finding a line of best fit. By fitting the 'line' and determining the intercept and slope coefficients, all we need to make a 'guess' about what the outcome will be for a person not included in the original sample are the predictor variables that were used to fit the regression model.

</exercise>

<exercise id="2" title="Setting up a prediction problem">

Let's consider an example. We will work with a dataset of 526 patients with dementia who were admitted to hospital. During the hospital stay, nurses rated their quality of life using a validated scale called the QALIDEM. Additional information about the QUALIDEM can be downloaded <a href="http://www.emgo.nl/files/2158">here</a>. The total quality of life score ranges from 0 to 100 with higher scores indicating better quality of life. This quality of life score is the 'outcome' that we are interested in being able to predict. The reason being is that it would be adventageous to be able to predict this quality of life score from other information that is already routinely collected as part of routine practice as opposed to having to allocate the additional time and resources required to undertake the quality of life scoring procedure. In this regard, we have available several predictor variables that we can use to predict quality of life, which are routinely collected in clinical practice. These are:

- Age
- Sex
- Severity of dementia as defined by categorization of a mini-mental status examination
- Length of stay in hospital
- The Pittsburgh Agitation Scale score
- If physical restraints were used during admission
- If chemical restraints were used during admission
- Barthel score (index of ability to independently perform various activities of daily living)

</exercise>

<exercise id="3" title="Training and test sets">

The first important step in setting up a machine learning approach to prediction, is to partition our dataset into two components called the 'training' set and the 'test' set. The 'training' set will contain 75% of the participants and this is the data we  will use to fit our model. The 'test' set containing the remaining 25% of the data will be set aside so that we can use it to determine how accurately our model can predict quality of life scores.

</exercise>


<exercise id="4" title="Candidate models">

One way machine learning approaches differ to the standard approach to statistical analyses, is that it is typical (and recommended) to evaluate several different 'candidate models' and determine which one provides the best predictions. There are literally hundreds of different models and algorithms that can be used to predict an outcome measured on a numerical scale (our outcome - QoL - is a numerical outcome). In our analysis let's select a basic linear regression model, as well as a model called a 'random forest' and another model called 'xgboost'. The specifics of how these particular models work is not important for you to know.

</exercise>


<exercise id="5" title="Cross-validation for model selection">


In order to be able to select which model is the best from our selection of candidates, it is best practice to use an approach known as 'cross-validation'. In cross-validation, we are subsetting our 'training' data into different sets so that we can use some of it to train the model and some of it to determine how well that model predicted the outcome.  The diagram below shows how this proceeds in a process of cross-validation. Each row represents the training data. Within each row are 20 squares. The yellow squares indicate the portion of data used to 'train' the model and the green squares indicate the portion of data used to 'test' how well the model performed. RMSE stands for 'root mean square error'. I will explain more about this later, but in simple terms it is a way to measure how accurately the model predicted the outcome. What you should take from this diagram is that in each iteration of the cross-validation procedure there are different portions of the data being used to train the model. 

<center>
<img src=/cross-validation.png />
</center>

<sub>Source: <a href="https://alison.rbind.io/talk/2020-rsc-tidyml/" >Introduction to Machine Learning with the Tidyverse</a><sub>

Let's take a look at this process in action. The table below shows some results from our training of the `xgboost` model to predict quality of life. The 'id' column shows which step of the cross validation process this result was produced from, the '.pred' column is the quality of life score predicted by the model (i.e. the 'predicted' value), the 'row' column is the number for the participant and 'QoL' was the actual quality of life score (i.e. the 'true value'). 

<center>
<img src=/cross-table.png />
</center>

One thing you might immediately notice from this table is that the 'predicted' value is sometimes quite close to the 'true' value, and sometimes it is quite different. The RMSE is a way that we can determine how well the model performs by essentially getting the average of the differences between the predicted and the true values (i.e. the error). The RMSE for the `xgboost` model was 12.27 and it is presented below. The other information that is important to take from this table is the 'n' column. This indicates the number of 'cross-validation' iterations from which the RMSE was calculated. In this case, we used 10 iterations.

<qu>Something to remember is that the *lower* the RMSE, the *better* the predictions were.</qu>

<center>
<h3>XGBoost model</h3>
<img src=/xg_metric.png />
</center>

Remember though that we said it was important to evaluate several different candidate models to determine which one was best. So let's go ahead and carry out the same approach using cross-validation for the 'random forest' and 'linear regression' models. The results are presented in the tables below.

<center>
<h3>Random forest model</h3>
<img src="/rf_metric.png" />
</center>

<center>
<h3>Linear regression model</h3>
<img src="/lm_metric.png" />
</center>

</exercise>

<exercise id="6" title="Selecting the best model exercise">

Out of the three models, which one performed the best? (i.e. predicted QoL the best)

<choice>

<opt text="XGBoost">

The model with the best predictions will have the lowest RMSE

</opt>

<opt text="Linear regression" correct=TRUE>

The linear regression model had the lowest RMSE

</opt>

<opt text="Random forest">

The model with the best predictions will have the lowest RMSE

</opt>

</choice>

</exercise>

<exercise id="7" title="Making predictions with 'new' data">

Now that we've determined that the linear regression model seems like it performs the best out of our selection of candidate models, we can go ahead and see how well it performs when we use it to predict quality of life scores with new data. We are able to do this because in our first step of the process we 'saved' some of the data to be our 'test' set. Saving data for validation of the model is a very important part of machine learning because it allows for the ability to determine the extent to which 'overfitting' may be a concern with the model we developed. We can diganose 'overfitting' of the model when the RMSE is a lot higher in our test set than what we found when we were training it. Say theoretically for example, if in our training data we found that the RMSE for predicting QoL score was 11 and then in our test set it was 20, this would mean that our model did not 'generalize' well from our training data to unseen data. This would mean it is not a good idea to start using this model in practice to predict quality of life scores for people with dementia. 

Let's take a look at how well the linear regression model we fitted to our training data performs on the new data we have stored away in our test set. There were 131 participants in our test set. Below is a table of just the first 10. The first column was the 'true' value for QoL, the second column ('.pred') is the quality of life score predicted by our model and the 'error' column is the difference between the two. The remaining columns are the 'predictor' variables that were used. 

<center>
<h3>Predictions on test set using the linear regression model</h3>
<img src="/lmpreds.png" />
</center>

Again we see that the difference between the predicted and true values vary quite a bit and we need to calculate the RMSE to give us an overall picture of how well the model performed. The result is in the table below.

<center>
<h3>Performance of the linear regression model on the test set</h3>
<img src="/lmtest.png" />
</center>

We can see from the table that the RMSE in the test set was similar to what we observed in the training data, which indicates the model generalized quite well to new data. Of course it is important now to determine if a model that on average is 'off' from the true value by about 10 points is accurate enough to be useful in clinical practice. 

<qu>There is a bit of contention around whether or not making predictions from simple linear regression models can be considered 'machine learning' though (even though the predictions made by the linear regression models were 'learned' from the data at hand). </qu>

</exercise>


<exercise id=8 title="Classification and Prediction">

The example on predicting quality of life scores that we went through above is an example of a 'prediction' machine learning problem. The other major approach to machine learning is called 'classification'. This is when instead of prediciting what a numerical score will be given the predictors included in the model, the goal is to predict which 'class' the outcome should be assigned. Image classification is one example of this type of approach to machine learning. One important thing to be aware of is that the way that the 'accuracy' of the predictions are evaluated differs between classification and prediction. For prediction we used the RMSE. For classification, we are interested in determing how often the classification predicted by the model were correct and incorrect. There are a variety of different ways to present this information, including the sensitivity and specificity of the model, true postitive, false positives, true negatives, false negatives, positive predictive values, negative predictive values the the area under the receiver operating characteristics curve (abbreviate to AUROC or ROC for short). 

The area under the curve is a very popular way that classification models are evaluated and you will come across plots like the one presented below in these sorts of papers. The slides presented below from this [presentation](https://conf20-intro-ml.netlify.com/slides/02-classifying.html), provide a good visual display of how to interpret the area under the curve (press right to scroll through just the 5 slides on ROC curves). Basically, a diagonal line straight across would indicate the model is no better than randomly guessing the classification. The closer the AUROC is to 1, the better the model is performing.

<iframe src="https://conf20-intro-ml.netlify.com/slides/02-classifying.html#135" width="100%" height="550px" allowfullscreen></iframe> 


</exercise>

<exercise id=9 title="Summary of important elements to look for in research using a machine learning approach">

1. The data were 'split' into training and testing (sometimes termed 'validation') sets
1. Several 'candidate' models were evaluated
1. Cross-validation techniques were used for model selection
1. The accuracy of predictions in the test set were reported to determine how well the model 'generalises' to new data (type will depend on if aim was 'prediction' or 'classification')

</exercise>

<exercise id="10" title="How does machine learning relate to artificial intelligence?">

The terms artificial intelligence and machine learning are sometimes used interchangeably, which is not exactly the case. This comic, although not specifically related to machine learning for healthcare applications, provides a simple breakdown to assist your understanding the terms related to machine learning and AI.


<a href="https://cloud.google.com/products/ai/ml-comic-1/" >Machine Learning: an online comic</a> 

</exercise>

<exercise id="11" title="Readings">

<qu>There are no 'required' readings this week. The readings are for your general interest.</qu>

## Reading

This paper is an example of a 'classification' problem. The authors used machine learning to train a model that is able to detect if an arterial pressure waveform is 'dampened' during coronary angiography, which is an indication that the catheter is positioned improperly and can increase risk of complications.

[Artificial Intelligence for Aortic Pressure Waveform Analysis During Coronary Angiography: Machine Learning for Patient Safety](https://www-sciencedirect-com.myaccess.library.utoronto.ca/science/article/pii/S1936879819314219)

This paper is another example of a 'classification' problem. The authors used machine learning to train a model to predict if pain management is a priority given presenting diagnosis.

[Identifying patients with pain in ED](https://www.aclweb.org/anthology/U19-1015.pdf)

This recent paper in JAMA is one of the few examples where the clinical 'usefulness' of a machine learning has been attempted to be evaluated. There are relatively few RCTs evaluating the effect of introducing an 'intervention' involving predictions from machine learning models in practice. You will see more and more of these over the years.

[Effect of a Machine Learning–Derived Early Warning System for Intraoperative Hypotension vs Standard Care on Depth and Duration of Intraoperative Hypotension During Elective Noncardiac Surgery: The HYPE Randomized Clinical Trial](https://jamanetwork-com.myaccess.library.utoronto.ca/journals/jama/fullarticle/2761469)

This systematic review attracted a lot of media attention because it highlights that lots of papers that have reported very promising results from deep learning approaches for image classification in healthcare were retrospective and many were reported with insufficient detail to allow for independent replication and evaluation.

[A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(19)30123-2/fulltext)
</exercise>

This paper is an example of a classification problem that is relevant to nursing practice and shows how the ROC curve was used to evaluate accuracy of the model. 

[Predicting Pressure Injury in Critical Care Patients: A Machine-Learning Model](http://web.b.ebscohost.com.myaccess.library.utoronto.ca/ehost/detail/detail?vid=1&sid=2b9a826c-9a1a-49f0-acd0-2e389cd31d43%40pdc-v-sessmgr01&bdata=JnNpdGU9ZWhvc3QtbGl2ZQ%3d%3d#AN=132756162&db=rzh)
