---
title: 'Week 3: Systemtatic reviews of randomized studies of interventions'
description:
  'The essential elements, advantages and disadvantages of systematic reviews of randomized studies of interventions will be discussed. You will be introduced to the statistical approach used to combine results from multiple RCTs, know as meta-analysis. We will cover the biases that threaten the validity of results from meta-analyses of RCTs. You will learn to understand and apply criteria for assessing the quality of evidence from meta-analyses using the GRADE framework.'
prev: /02rct
next: null
type: chapter
id: 4
---

<exercise id="1" title="Introduction">

The reason we are covering this topic is that we should really only be using the results of a meta-analysis of randomized studies of interventions to inform clinical practice decision-making within the GRADE framework **if** the methods of the systematic review were adequate. The content we cover should provide you with the knowledge to be able to confidently make that decision.

First of all, why do we do systematic reviews at all? Doing a systematic review enables: 

- Reduction of large quantities of information to key findings;
- Production of statements about evidence to guide clinical decision making (guidelines);
- Identification of gaps in research; and
- Through meta-analysis, increased power to detect small effects and a more accurate estimate of the true effect

So it's clear to see why it would be useful to use information from a systematic review (which may or may not include a meta-analysis) to inform our decision-making in regard to a particular PICO. Basically, it saves us from having to do a whole lot of work ourselves, such searching through all the literature to find the relevant articles, assessing their risk of bias and doing a meta-analysis. However, if that systematic review wasn't done well, then any subsequent decision could be based on incomplete or, even worse, incorrect information.

It's important to note from the outset that there are lots of different types of review articles and even lots of different types within the subset of systematic reviews. The categories below may be helpful in distinguishing between the different types.

|Type| Explanation|
|---|---|
|Review article| Broad term that includes may types of reviews (e.g. narrative reviews, etc.); often not systematically conducted|
|Scoping review| Addresses a broad topic; purpose is to identify what research exists (what has been studied) and where the gaps are; often includes “grey” literature.|
|Systematic review| A rigourous summary of research evidence that addresses a focused clinical question in a systematic, reproducible manner|
|Meta-synthesis| A systematic summary of qualitative research|

</exercise>

<exercise id="2" title="Systematic review process">


Regardless of the type of systematic review, they should all, in general, follow the main steps displayed in the graphic below.

<center>

<img src="/sr-process.png" />
</center>

The key point to note here is that there are steps that systematic reviewers can use to minimize the risk of bias associated with each of these stages within the process. For example, in the "search the literature" stage, reviewers need to minimize the risk that they 'miss' identifying studies that do meet the inclusion criteria for their review. One way they can minimize this risk is to ensure that they undertake comprehensive searches of the literature (at least 2 databases) and to report the methods they used in such a way that consumers of the review can assess how well they controlled for this risk (i.e. by including the list of keywords and terms used to undertake the literature search).

Depending on the type of review, different tools and approaches will be used. This week we focus specifically on systematic reviews of randomized studies of interventions. A major feature of these types of systematic reviews is the use of meta-analysis to combine results from the studies included in the review. The distinction in meaning between the terms systematic review and meta-analysis outlined below is essential to understand. Meta-analysis if just one of the 'tools' that a reviewer can use to synthesise the data (step 5 in the processs). 

<qu> A systematic review is not a meta–analysis! (but a meta-analysis can be part of a systematic review)
</qu>

In many circumstances it is not appropriate to undertake a meta-analysis of randomized studies of interventions. The main reasons for are that:

- more than one study that reported on the outcome was not identified; or
- the outcome was measured differently in the studies included in the review.



### PRISMA statement

You may have heard of the PRISMA statement. This statement is an evidence-based minimum set of items for reporting of systematic reviews and meta-analyses (title, abstract, introduction, methods, results, discussion, funding). Researchers undertaking systematic reviews and meta-analyses are encouraged to use this checklist to guide them in the reporting of both protocols [(PRISMA-P)](https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/2046-4053-4-1) and results of systematic reviews [(PRISMA)](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1000100). One thing to note (that even systematic review authors seem confused about) is that these statements are to assist with the *reporting* of systematic reviews. They are not methodological guides. So, for example, it is not appropriate for a systematic review author to report that they "used PRISMA to guide the design of the protocol". 

Fortunately, a tool does exist that we can use to evaluate the extent to which systematic review authors were able to minimize the risk of bias that may be introduced with different aspects fo the systematic review process. This is called the AMSTAR 2 tool. I will refer to this later in the chapter and also demonstrate it's use in the webinar this week. But first, I'll introduce you to some of the core concepts that it covers.

</exercise>

<exercise id="3" title="Systematic review methods that signal 'quality'">

As mentioned earlier in regard to comprehensive literature searching, there are several strategies that reviewers can use to minimize bias assciated with the different activitie involved in the systematic review process.

### Duplicate study selection, data extraction, risk of bias assessment

One of these is undertaking study selection, data abstraction (another term used is data extraction) and critical appraisal (otherwise termed risk of bias assessment) in duplicate. It is best practice for two *independent* reviewers to undertake these steps and compare between the two for any inconsistencies that can be corrected. Last semester I introduced you to Covidence, which is a tool developed specifically to assist reviewers with this task. If a review did not undertake study selection, data extraction and risk of bias assessment in duplicate, there is a much higher risk that errors could have occurred. If these tasks were not undertaken in duplicate, it may not be wise to use the result of a meta-analsis based on that information in your decision-making, as it is very likely to be biased.


### Use of a satisfactory technique to assess risk of bias

We used the RoB2 tool to assess risk of bias for a randomized study of an intervention last week. This is an example of an accepted technique to assess risk of bias in a systematic review. Other 'tools' for assessing risk of bias do exist. Item 9 of the AMSTAR tool addresses the components of RoB assessment that should be included in a systematic review. 

<qu> Note: The CASP tools are **not** tools to assess risk of bias. They are critical appraisal tools. CASP tools cannot be used within GRADE. </qu>

### Clear descriptions of included studies

Just as we learnt last week in the context of RCTs and the importance of reporting of intervention components (TIDieR checklist), it is vital for a systematic review to report the important details about the studies included in the review. This should include descriptions for the population, intervention, comparators, outcomes as well as aspects of the research design including the sample size/outcome measurements etc. Although very useful to have, I would not describe the lack of an exhaustive description of the included studies as a 'fatal flaw' meaning that you should disregard the information from the systematic review entirely. This is because it is not that difficult to refer to the individual study for the additional detail that you may need. 

</exercise>

<exercise id="4" title="Forest plots">

I'd now like to introduce you to meta-analysis. Meta-analysis is a type of statistical analysis used to combine the results from multiple studies into one overall estimate. The typical way for a meta-analysis to be presented in a systematic review is a forest plot. 

# Forest plot for a binary outcome

Below is a demonstration of a forest plot for a binary outcome. Binary outcomes have 2 levels, so they are quite common in clinical research (i.e. dead or alive, hospitalized or not, infection or no infection).

<img src="/fp-binary.png" />

<sub>
Source: Reid K 2005. Interpreting and understanding meta-analysis graphs. Australian Family Physician, 35 , 8, 635-638.
</sub>
<br>
<br>
The Forest plot provides the summary data entered for each study on the left hand side, including how many participants were included in each group and how many experienced the outcome (n/N).

The main results of each study are presented graphically in the center and numerically in the right hand section of the plot. There is a black box presented for each study. The mid-point of the box represents the effect estimate. For binary outcomes in randomized studies of interventions, this would most commonly be the relative risk (RR). The area of the box represents the weight given to the study (the larger the box - the more 'weight' that study contributed to the overall meta-analysis result). This is designed so that eyes are drawn towards the studies that are given more weight. The width of the lines extending from each side of the boxes shows the confidence intervals of the effect estimate for individual studies. 

The main result for the meta-analysis is presented in the bottom row (indicated as "Total"). Again, the result is presented graphically (in the center) and numerically (on the right). The center-point of the diamond indicates the effect estimate and the width of the diamond shows the confidence intervals. 

Other important information presented in a forest plot includes:

1. The statistical significance of the result (here presented as the 'Test for overall effect'); and
1. Information on how consistent results were *between* studies (here presented as the 'Test for heterogeneity'). For now, just note where this information is presented in the forest plot of the meta-analysis. We will cover heterogeneity in a later section. 


# Forest plot for a continuous outcome

Below is a demonstration of a forest plot for a continuous outcome. 

<img src="/fp-continuous.png" />

<sub>
Source: Reid K 2005. Interpreting and understanding meta-analysis graphs. Australian Family Physician, 35 , 8, 635-638.
</sub>
<br>
<br>

A difference to note is that there is a choice that reviewers can make in how the effect estimate for the meta-analysis is conducted. If all of the included studies measured the outcome using the same measurement instrument, then it is typical to calculate a pooled mean difference (also termed 'weighted mean difference' because in meta-analysis the individual studies are 'weighted' as to how much they contribute to the overall effect). If the individual studies used different measurement instruments to measure the same outcome, then the reviewers would need to calculate the 'standardized mean difference' instead. If the standardized mean difference is used, it is important to take this into consideration when interpreting the results. The simplest interpretation is to use 'rules of thumb' for the effect sizes [(Cochrane Handbook, 2020)](https://handbook-5-1.cochrane.org/chapter_12/12_6_2_re_expressing_smds_using_rules_of_thumb_for_effect_sizes.htm_). One example is that 0.2 represents a small effect, 0.5 a moderate effect, and 0.8 a large effect (Cohen 1988). 


</exercise>


<exercise id="5" title="Forest plot exercise 1">

<center>
<img src="/fp-test.png" width ="400px"/>
</center>

How many studies were included in the meta-analysis?<br><br>

<choice>

<opt text="Five" correct="true">

Good job! Each square box represents an effect estimate from a single study.

</opt>

<opt text="Six">

The diamond at the bottom represents the result from the meta-analysis - not an individual study.

</opt>

</choice>


</exercise>

<exercise id="6" title="Forest plot exercise 2">

<center>
<img src="/fp-test.png" width ="400px"/>
</center>

Which was the largest study?<br><br>

<choice>

<opt text="Second from the top">

This study certainly contributed the most weight to the overall meta-analysis (36.25%), but we cannot be sure from the information presented if it necesarrily had the biggest sample size. In a meta-analysis of a continuous outcome, typically the 'inverse-variance' approach is used to weight the studies. This means that the studies with the least variance are wieghted the heighest - hence the term 'inverse variance'. It is not always the case that the largest study in terms of sample size is 'weighted' the highest. So we really can't tell for sure which of these studies had the largest sample size.   

</opt>

<opt text="The bottom black square">

From the graphic, it does look like the botttom black square may be the biggest, but it's actually quite hard to distinguish any differences in sizes between the bottom four. So we have to be careful when interpreting results from forest plots and always consult the numbers too. 

</opt>

<opt text="Not enough information provided" correct="true">

Well done. In a meta-analysis of a continuous outcome, typically the 'inverse-variance' approach is used to weight the studies. This means that the studies with the least variance are wieghted the heighest - hence the term 'inverse variance'. It is not always the case that the largest study in terms of sample size is 'weighted' the highest. So we really can't tell for sure which of these studies had the largest sample size.   

</opt>

</choice>


</exercise>

<exercise id="7" title="Forest plot exercise 3">

<center>
<img src="/fp-test.png" width ="400px"/>
</center>

What is the result of the meta-analysis?<br><br>

<choice>

<opt text="Treatment is better" correct="true">

The middle of the diamond at the bottom (i.e. the effect estimate) is on the side that indicates the meta-analysis 'favours treatment' and it does not reach the line in the middle indicating no difference between groups. Of course we need additional information to know if a difference of about 15-25 points is 'clinically significant' or not. But from the information we have been presented here, we can conclude that treatment is better.

</opt>

<opt text="Control is better">

</opt>

</choice>


</exercise>


<exercise id="8" title="Heterogeneity">

As you will recall from what we have previously learnt about the GRADE framework, it is important to consider how *consistent* the effects of an intervention are, when deriving a recommendation for practice. A meta-analysis can show you some information about whether or not there is an issue with regard to inconsistency. Consider the diagram below that shows the results of nine different individual studies combined in a meta-analysis. We can never expect that the results of individual studies should be exactly the same - the individual estimates of treatment effect will vary by chance, because of randomisation and also because they are conducted in slightly different populations in different environments/contexts etc. So some variation is expected. Here we see that the estimates in the individual studies do vary. The question is whether there is more variation than would be expected by chance alone. When excessive variation occurs, we can describe this as 'statistical heterogeneity', or just 'heterogeneity'. 

<img src="https://adc.bmj.com/content/archdischild/90/8/845/F2.large.jpg?width=800&height=600&carousel=1" />
<br><br>
<a href="https://adc.bmj.com/content/90/8/845"><sub>Source: Akobeng, A. K. (2005). Understanding systematic reviews and meta-analysis. Archives of disease in childhood, 90(8), 845-848.</sub>
</a>
<br><br>

You will come across two different (but related) ways of assessing heterogeneity in meta-analyses of randomized studies of interventions - the chis-squared test for heterogeneity and the I<sup>2</sup>. You can think of the chi-square test like you would a normal p-value. If the chi-square test p-value is 'significant' then the variation in results between the studies included in the meta-analysis was more than we would have expected from just normal random variation alone and therefore the meta-analysis could be described as 'inconsistent'. The I<sup>2</sup> statistic can be thought of as 'quantifying the amount' of heterogeneity. Typically values over 50% would be considered a large amount of heterogeneity and we would therefore be concerned about **inconsistency**. Downgrading the quality of evidence due to concerns about inconsistency should be considered for high values of I<sup>2</sup> or if the chi-square test is significant.

</exercise>


<exercise id="9" title="Reporting bias">

The credibility of results from a systematic review can be compromised by reporting biases. A reporting bias is when dissemination of research findings is influenced by the nature of the results [(McGauran et al., 2010)](https://trialsjournal.biomedcentral.com/articles/10.1186/1745-6215-11-37). In simpler terms, studies in which interventions are shown to be ineffective are sometimes not published, meaning that only a portion of evidence is available. "Publication bias" is an interchangeable term for reporting bias. It means the same thing.

### Determining risk of reporting bias in a meta-analysis for GRADE assessments

In making a decision about the quality of evidence for an outcome using GRADE, we need to consider the potential for reporting bias. If you are making your GRADE assessment for an outcome where evidence from a meta-analysis exists, you should also look to see if the reviewers investigated the potential for reporting bias using graphical or statistical tests. The 'graphical' test that may have been used is called a 'funnel plot'. It looks like this:

<img src="https://www.bmj.com/content/bmj/343/bmj.d4002/F1.large.jpg?width=800&height=600" />
<br>

<sub><a href="https://www.bmj.com/content/343/bmj.d4002">Source: Recommendations for examining and interpreting funnel plot asymmetry in meta-analyses of randomised controlled trials BMJ 2011; 343 doi: https://doi.org/10.1136/bmj.d4002</a></sub>
<br>
<br>
Each dot in the plot represents a result from an individual study included in the meta-analysis. It plots the effect estimate (in this case OR but could be RR or mean difference) on the horizontal axis and the standard error (a measure of the precision) of that effect estimate is on the vertical axis. This way we can compare the effect estimates of studies that were highly precise (which are typically the larger studies) against those that were imprecise (typically small studies). The plot shown here is an example of a 'symmetrical' funnel plot, which indicates a low risk of reporting bias. If the funnel plot were asymmetric (i.e. spaces within the triangle devoid of 'dots'), this would indicate a potential for reporting bias. 

Identifying repoerting bias using graphical and statistical techniques is a complex issue. For example, it is not always possible to test for funnel plot asymmetry. For example, if fewer than 10 studies are included in a meta-analysis, the power of the test is too low to distinguish chance from real funnel plot asymmetry. 

<qu>For your purposes, it is enough to know that reporting bias is an issue and to check to see 'if' and 'how' reviewers determined if the result of a meta-analysis was at high risk of bias due to reporting bias. If it is a meta-analysis of a large number of studies and the reviewers do not mention the risk of reporting bias, then this may raise our concerns.</qu>

</exercise>

<exercise id="10" title="Critical appraisal of systematic reviews of randomized studies of interventions">

To recap, the reason we are covering this topic is that we should really only be using the results of a meta-analysis of randomized studies of interventions can to inform clinical practice decision-making within the GRADE framework **if** the methods of the systematic review were sufficient. What you need to be able to do, is decide **if** the systematic review was conducted in such a way that the result of a meta-analysis **can be trusted enough** to be included in your decision-making. So for example, if in your literature search you identify a systematic review with a meta-analysis that met the criteria for your PICO question but the systematic review **did not** assess the risk of bias of the individual studies included in the review, then it would **not** be appropriate to use that meta-analysis result in your decision-making. You would instead need to undertake the additional work yourself of assessing risk of bias of all the included studies. Likewise, if there was an issue with the search strategy for the systematic review (e.g. only searched one database), then we have to think carefully about whether or not it would be appropriate to use a meta-analysis result from that review because it is possible that studies that fit the PICO criteria may not have been uncovered by the literature search and were therefore not able to be included in the meta-analysis. 

<qu>One very useful resource that can assist you to make this decision is the AMSTAR 2 tool. It is a critical appraisal tool for systematic reviews of interventions.</qu>

A reading for this week is the paper explaining the components of AMSTAR 2 tool. 

:book: <a href= "https://www-bmj-com.myaccess.library.utoronto.ca/content/358/bmj.j4008">AMSTAR 2: a critical appraisal tool for systematic reviews that include randomised or non-randomised studies of healthcare interventions, or both</a>

:book: <a href= "https://www-bmj-com.myaccess.library.utoronto.ca/content/bmj/suppl/2017/09/21/bmj.j4008.DC1/sheb036104.ww1.pdf">AMSTAR 2 guidance document</a>

<iframe src="https://amstar.ca/docs/AMSTAR-2.pdf" width="100%" height="1050" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>
</exercise>

:link: <a href="https://amstar.ca/Amstar_Checklist.php">Online version of AMSTAR 2 checklist</a>

<exercise id="11" title="Webinar on critical appraisal of systematic reviews of randomized studies of interventions">

I will host a webinar on Friday 24th January at 12md to demonstrate critical appraisal of the systemtatic review linked below. Please attend if you can, as it is good for me to be able to answer questions as they arise. We will use the AMSTAR criteria as a guide for the appraisal. 

:link: <a href="https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD008462.pub3/full?highlightAbstract=venous%7Cwithdrawn%7Ccannula%7Ccannul">Heparin versus 0.9% sodium chloride locking for prevention of occlusion in central venous catheters in adults</a>


</exercise>


<exercise id="12" title="Readings">

## Required

Gray, J.R., Grove, S.K. & Sutherland, S.  (2017). The Practice of Nursing Research (8th ed.)  Chapter 19: Evidence synthesis and strategies for evidence-based practice (Read pp. 453-479).

Smith, R.L. & Jones, L.V. (2012). Using evidence from systematic reviews. In J.V. Craig & R.L. Smyth (Eds.) The Evidence-based Practice Manual for Nurses (3rd ed.) (187 – 213.)

## Example articles

<a href="https://onlinelibrary-wiley-com.myaccess.library.utoronto.ca/doi/epdf/10.1111/jan.13509">Song, M., Li, N., Zhang, X., Shang, Y.,Yan, L., Chu, J., Sun, R., & Xu, Y. (2018). Music for reducing the anxiety and pain of patients undergoing a biopsy: A meta-analysis. Journal of Advanced Nursing, 74, 1016-1029.</a>

<a href="https://journals-scholarsportal-info.myaccess.library.utoronto.ca/details/00207489/v63icomplete/201_doamdpaasram.xml">Depth of anaesthesia monitoring during procedural sedation and analgesia: A systematic review and meta-analysis</a>

## Additional reading

<a href="https://adc.bmj.com/content/90/8/845.full">Akobeng, A. (2005). Understanding systematic reviews and meta-analysis. Archives of Disease in Childhood, 90, 845-848.</a>

<a href="https://journals-scholarsportal-info.myaccess.library.utoronto.ca/details/08954356/v62i0010/1006_prifsramtps.xml">Moher, D., Liberati, A., Tetzlaff, J., Altman, D.G., The PRISMA Group. (2009). Preferred reporting items for systematic reviews and meta-analyses: the PRISMA statement. Journal of Clinical Epidemiology, 62, 1006 – 1012.</a>

<a href="https://pdfs.semanticscholar.org/fac5/31c851fe5bb28454c0c4395dafe8bd846a17.pdf"> Interpreting and understanding meta-analysis graphs. </a>


</exercise>